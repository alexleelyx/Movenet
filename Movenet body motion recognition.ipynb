{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5c21ec-e2fe-4122-ba84-60e2cd4ba4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.4.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: opencv-python in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: matplotlib in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (3.19.6)\n",
      "Requirement already satisfied: six~=1.15.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: gast==0.3.3 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.37.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.32.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (2.10.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.19.5)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.14.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (65.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.28.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.11.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard~=2.4->tensorflow==2.4.1) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./opt/anaconda3/envs/Movenet/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8494c2-2777-442b-8a77-39310b8ad246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ee90a75-b6b9-4630-b0ab-e3ee6e8ae331",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"lite-model_movenet_singlepose_lightning_3.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "699c08f8-0644-45bf-814c-29e182bcbe68",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make predictions \u001b[39;00m\n\u001b[1;32m     15\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mset_tensor(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(input_image))\n\u001b[0;32m---> 16\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m keypoints_with_scores \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Rendering \u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Movenet/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py:540\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_safe()\n\u001b[0;32m--> 540\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Rendering \n",
    "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    \n",
    "    cv2.imshow('we', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839a90d6-3876-4df3-a1cd-b6822dc88b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (255,255,0), -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cfb9bcd-eb94-4081-98b7-19701b4ecac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b50a9e-f9df-4144-b7ed-f98c90e35bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255,255,255), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aebce1-2c5d-40e7-bce1-626cb1955a4a",
   "metadata": {},
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img,axis=0),192,192)\n",
    "    input_image = tf.cast(img,dtype=tf.float32)\n",
    "    cv2.imshow('MoveNet Lightning',frame)\n",
    "    while cv2.waitKey(10)&0xFF==ord(\"m\"):\n",
    "        break\n",
    "cap.realease\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48c55c-5098-410c-a57b-538961ee3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"IMG_3728.MOV\")\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    input_image = tf.cast(img, dtype=tf.float32)\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Rendering \n",
    "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    \n",
    "    cv2.imshow('MoveNet Lightning', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed770868-b94c-4a9c-b419-a3ba73a3095c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3aa5c-5924-4641-a618-878a1463d060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52e0f9-c4c1-4de4-8533-eacd75599652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80906a-8ff7-4809-a472-ae628e264c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d5587-f855-4cb4-9f74-dad6c4c50d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
